# soulgenesis_128_real.py
import streamlit as st
import torch
import torch.nn as nn
import torch.nn.functional as F
from torchvision import datasets, transforms
from torch.utils.data import DataLoader, Dataset
import numpy as np
import matplotlib.pyplot as plt
import os
from PIL import Image
import zipfile
import io

# Set page config
st.set_page_config(
    page_title="SoulGenesis 128 - Real Image Compression", 
    page_icon="ðŸ“¸",
    layout="wide"
)

# Custom CSS
st.markdown("""
<style>
    .main-header {
        font-size: 3rem;
        color: #6c5ce7;
        text-align: center;
        margin-bottom: 2rem;
    }
    .sub-header {
        font-size: 1.5rem;
        color: #a29bfe;
        margin-bottom: 1rem;
    }
    .metric-box {
        background-color: #2d3436;
        padding: 1rem;
        border-radius: 10px;
        margin: 0.5rem 0;
    }
    .success-box {
        background-color: #00b894;
        padding: 1rem;
        border-radius: 10px;
        margin: 1rem 0;
    }
</style>
""", unsafe_allow_html=True)

# Enhanced CNN Model for 128x128
class EnhancedEncoder(nn.Module):
    def __init__(self, soul_dim=512, in_channels=3):
        super().__init__()
        self.net = nn.Sequential(
            # 128x128 â†’ 64x64
            nn.Conv2d(in_channels, 64, 4, stride=2, padding=1),
            nn.BatchNorm2d(64),
            nn.LeakyReLU(0.2, inplace=True),
            
            # 64x64 â†’ 32x32
            nn.Conv2d(64, 128, 4, stride=2, padding=1),
            nn.BatchNorm2d(128),
            nn.LeakyReLU(0.2, inplace=True),
            
            # 32x32 â†’ 16x16
            nn.Conv2d(128, 256, 4, stride=2, padding=1),
            nn.BatchNorm2d(256),
            nn.LeakyReLU(0.2, inplace=True),
            
            # 16x16 â†’ 8x8
            nn.Conv2d(256, 512, 4, stride=2, padding=1),
            nn.BatchNorm2d(512),
            nn.LeakyReLU(0.2, inplace=True),
            
            # 8x8 â†’ 4x4
            nn.Conv2d(512, 512, 4, stride=2, padding=1),
            nn.BatchNorm2d(512),
            nn.LeakyReLU(0.2, inplace=True),
            
            nn.Flatten(),
            nn.Linear(512 * 4 * 4, soul_dim),
            nn.Tanh()  # Keep values in reasonable range
        )
    
    def forward(self, x):
        return self.net(x)

class EnhancedDecoder(nn.Module):
    def __init__(self, soul_dim=512, out_channels=3):
        super().__init__()
        self.initial_size = 4
        self.initial_channels = 512
        
        self.fc = nn.Sequential(
            nn.Linear(soul_dim, self.initial_channels * self.initial_size * self.initial_size),
            nn.LeakyReLU(0.2, inplace=True)
        )
        
        self.net = nn.Sequential(
            # 4x4 â†’ 8x8
            nn.ConvTranspose2d(512, 512, 4, stride=2, padding=1),
            nn.BatchNorm2d(512),
            nn.LeakyReLU(0.2, inplace=True),
            
            # 8x8 â†’ 16x16
            nn.ConvTranspose2d(512, 256, 4, stride=2, padding=1),
            nn.BatchNorm2d(256),
            nn.LeakyReLU(0.2, inplace=True),
            
            # 16x16 â†’ 32x32
            nn.ConvTranspose2d(256, 128, 4, stride=2, padding=1),
            nn.BatchNorm2d(128),
            nn.LeakyReLU(0.2, inplace=True),
            
            # 32x32 â†’ 64x64
            nn.ConvTranspose2d(128, 64, 4, stride=2, padding=1),
            nn.BatchNorm2d(64),
            nn.LeakyReLU(0.2, inplace=True),
            
            # 64x64 â†’ 128x128
            nn.ConvTranspose2d(64, out_channels, 4, stride=2, padding=1),
            nn.Sigmoid()
        )
    
    def forward(self, x):
        x = self.fc(x)
        x = x.view(-1, self.initial_channels, self.initial_size, self.initial_size)
        return self.net(x)

class SoulGenesis128(nn.Module):
    def __init__(self, soul_dim=512):
        super().__init__()
        self.encoder = EnhancedEncoder(soul_dim)
        self.decoder = EnhancedDecoder(soul_dim)
    
    def forward(self, x):
        soul_seed = self.encoder(x)
        reconstructed = self.decoder(soul_seed)
        return reconstructed, soul_seed

# Simple 128x128 dataset (since we can't download on Streamlit)
class Simple128Dataset(Dataset):
    def __init__(self, num_samples=500, img_size=128):
        self.num_samples = num_samples
        self.img_size = img_size
        self.data = self._generate_128_images()
    
    def _generate_128_images(self):
        images = []
        for i in range(self.num_samples):
            img = np.zeros((3, self.img_size, self.img_size), dtype=np.float32)
            pattern_type = i % 8
            
            # More complex 128x128 patterns
            if pattern_type == 0:  # Multi-colored circles
                for channel in range(3):
                    y, x = np.ogrid[-self.img_size//2:self.img_size//2, -self.img_size//2:self.img_size//2]
                    radius = self.img_size // (4 + channel)
                    mask = x*x + y*y <= radius*radius
                    img[channel, mask] = 1.0 - (channel * 0.3)
                    
            elif pattern_type == 1:  # Gradient patterns
                for row in range(self.img_size):
                    for col in range(self.img_size):
                        img[0, row, col] = row / self.img_size  # Red gradient
                        img[1, row, col] = col / self.img_size  # Green gradient
                        img[2, row, col] = (row + col) / (2 * self.img_size)  # Blue gradient
                        
            elif pattern_type == 2:  # Checkerboard
                square_size = self.img_size // 8
                for row in range(self.img_size):
                    for col in range(self.img_size):
                        if (row // square_size + col // square_size) % 2 == 0:
                            img[0, row, col] = 1.0  # Red
                            img[1, row, col] = 0.5  # Green
                            
            elif pattern_type == 3:  # Radial patterns
                center = self.img_size // 2
                for row in range(self.img_size):
                    for col in range(self.img_size):
                        dist = np.sqrt((row - center)**2 + (col - center)**2)
                        intensity = 1.0 - (dist / (self.img_size//2))
                        img[0, row, col] = max(0, intensity)  # Red
                        img[1, row, col] = max(0, intensity * 0.7)  # Green
                        img[2, row, col] = max(0, intensity * 0.4)  # Blue
                        
            elif pattern_type == 4:  # Wave patterns
                for row in range(self.img_size):
                    for col in range(self.img_size):
                        wave1 = 0.5 + 0.5 * np.sin(row * 0.1)
                        wave2 = 0.5 + 0.5 * np.sin(col * 0.1)
                        wave3 = 0.5 + 0.5 * np.sin((row + col) * 0.05)
                        img[0, row, col] = wave1
                        img[1, row, col] = wave2
                        img[2, row, col] = wave3
                        
            elif pattern_type == 5:  # Geometric shapes
                # Multiple shapes in one image
                y, x = np.ogrid[-self.img_size//2:self.img_size//2, -self.img_size//2:self.img_size//2]
                
                # Circle
                circle_mask = x*x + y*y <= (self.img_size//6)**2
                img[0, circle_mask] = 1.0
                
                # Square
                square_size = self.img_size // 4
                square_start = self.img_size // 3
                square_slice = slice(square_start, square_start + square_size)
                img[1, square_slice, square_slice] = 1.0
                
                # Diagonal line
                for i in range(self.img_size):
                    if 0 <= i < self.img_size:
                        img[2, i, i] = 1.0
                        img[2, i, self.img_size - i - 1] = 1.0
                        
            else:  # Complex noise patterns
                noise = np.random.rand(3, self.img_size, self.img_size)
                img = noise * 0.8 + 0.1  # Keep in reasonable range
            
            images.append(img)
        
        return torch.tensor(images)
    
    def __len__(self):
        return self.num_samples
    
    def __getitem__(self, idx):
        return self.data[idx], idx % 8

# Training function with advanced features
def train_128_model(model, train_loader, epochs, lr, device):
    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=1e-6)
    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, epochs)
    
    train_losses = []
    best_loss = float('inf')
    
    progress_bar = st.progress(0)
    status_text = st.empty()
    loss_chart = st.empty()
    
    for epoch in range(epochs):
        model.train()
        epoch_loss = 0.0
        batch_count = 0
        
        for imgs, _ in train_loader:
            imgs = imgs.to(device)
            optimizer.zero_grad()
            recon, _ = model(imgs)
            
            # Combined loss for better quality
            l1_loss = F.l1_loss(recon, imgs)
            mse_loss = F.mse_loss(recon, imgs)
            loss = l1_loss + 0.5 * mse_loss  # Weighted combination
            
            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)  # Gradient clipping
            optimizer.step()
            
            epoch_loss += loss.item() * imgs.size(0)
            batch_count += 1
        
        scheduler.step()
        
        if batch_count > 0:
            epoch_loss /= len(train_loader.dataset)
            train_losses.append(epoch_loss)
            
            if epoch_loss < best_loss:
                best_loss = epoch_loss
        
        # Update UI
        progress = (epoch + 1) / epochs
        progress_bar.progress(progress)
        current_lr = optimizer.param_groups[0]['lr']
        status_text.text(f"Epoch {epoch+1}/{epochs} - Loss: {epoch_loss:.4f} - Best: {best_loss:.4f} - LR: {current_lr:.2e}")
        
        # Update chart
        if len(train_losses) > 1:
            fig, ax = plt.subplots(figsize=(10, 4))
            ax.plot(train_losses, marker='o', color='#6c5ce7', linewidth=2)
            ax.set_xlabel('Epoch')
            ax.set_ylabel('Combined Loss')
            ax.set_title('Training Progress')
            ax.grid(True, alpha=0.3)
            loss_chart.pyplot(fig)
            plt.close(fig)
    
    return train_losses, best_loss

# Compression utilities
def compress_to_bytes(soul_seed, quantize_bits=8):
    """Convert soul seed to actual bytes"""
    if quantize_bits == 8:
        # Scale to [-128, 127] range for 8-bit integers
        scaled = (soul_seed * 127).clamp(-128, 127)
        quantized = scaled.round().to(torch.int8)
        return quantized.numpy().tobytes()
    else:
        # 16-bit for higher precision
        scaled = (soul_seed * 32767).clamp(-32768, 32767)
        quantized = scaled.round().to(torch.int16)
        return quantized.numpy().tobytes()

def decompress_from_bytes(byte_data, quantize_bits=8, device='cpu'):
    """Reconstruct soul seed from bytes"""
    if quantize_bits == 8:
        quantized = torch.tensor(np.frombuffer(byte_data, dtype=np.int8), device=device)
        return quantized.float() / 127.0
    else:
        quantized = torch.tensor(np.frombuffer(byte_data, dtype=np.int16), device=device)
        return quantized.float() / 32767.0

# Main app
def main():
    st.markdown('<h1 class="main-header">ðŸ“¸ SoulGenesis 128 - Real Image Compression</h1>', unsafe_allow_html=True)
    
    st.sidebar.header("Configuration")
    soul_dim = st.sidebar.slider("Soul Seed Dimension", 256, 1024, 512)
    epochs = st.sidebar.slider("Training Epochs", 5, 30, 15)
    batch_size = st.sidebar.slider("Batch Size", 4, 16, 8)
    learning_rate = st.sidebar.slider("Learning Rate", 1e-4, 5e-3, 1e-3, format="%.4f")
    num_samples = st.sidebar.slider("Training Samples", 200, 1000, 400)
    quantize_bits = st.sidebar.selectbox("Quantization", [8, 16], index=0)
    
    device = "cpu"
    st.sidebar.info(f"ðŸš€ 128x128 Resolution - {soul_dim}D Soul Seeds")
    
    # Create dataset
    @st.cache_resource
    def create_128_dataset(_num_samples):
        return Simple128Dataset(num_samples=_num_samples, img_size=128)
    
    dataset = create_128_dataset(num_samples)
    train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)
    
    # Initialize model
    model = SoulGenesis128(soul_dim=soul_dim).to(device)
    
    # Model info
    param_count = sum(p.numel() for p in model.parameters())
    st.sidebar.info(f"Model Size: {param_count:,} parameters")
    
    # Compression metrics
    original_size = 3 * 128 * 128  # 49,152 values
    compressed_size = soul_dim * (quantize_bits / 8)  # Bytes for soul seed
    compression_ratio = original_size * 4 / compressed_size  # Assuming 32-bit floats originally
    
    col1, col2, col3 = st.columns(3)
    with col1:
        st.markdown('<div class="metric-box">', unsafe_allow_html=True)
        st.metric("Input Resolution", "128Ã—128Ã—3")
        st.metric("Input Values", f"{original_size:,}")
        st.markdown('</div>', unsafe_allow_html=True)
    
    with col2:
        st.markdown('<div class="metric-box">', unsafe_allow_html=True)
        st.metric("Soul Seed Size", f"{soul_dim} values")
        st.metric("Quantization", f"{quantize_bits}-bit")
        st.markdown('</div>', unsafe_allow_html=True)
    
    with col3:
        st.markdown('<div class="metric-box">', unsafe_allow_html=True)
        st.metric("Compression Ratio", f"{compression_ratio:.1f}:1")
        st.metric("Theoretical", f"{compressed_size:.1f} bytes")
        st.markdown('</div>', unsafe_allow_html=True)
    
    # Sample preview
    st.markdown('<div class="sub-header">ðŸŽ¨ 128x128 Training Samples</div>', unsafe_allow_html=True)
    sample_images = dataset.data[:6].permute(0, 2, 3, 1).numpy()
    cols = st.columns(6)
    for i, col in enumerate(cols):
        with col:
            st.image(sample_images[i], use_column_width=True)
            st.caption(f"Sample {i+1}")
    
    # Training
    if st.button("ðŸš€ Train 128x128 Model", type="primary"):
        st.markdown('<div class="sub-header">ðŸ“ˆ Training 128x128 Compression</div>', unsafe_allow_html=True)
        
        with st.spinner("Training advanced 128x128 compressor..."):
            train_losses, best_loss = train_128_model(model, train_loader, epochs, learning_rate, device)
        
        st.markdown('<div class="success-box">', unsafe_allow_html=True)
        st.success(f"âœ… Training Complete! Best Loss: {best_loss:.4f}")
        st.markdown('</div>', unsafe_allow_html=True)
        
        # Test compression
        st.markdown('<div class="sub-header">ðŸ”® Compression Demo</div>', unsafe_allow_html=True)
        
        model.eval()
        with torch.no_grad():
            test_batch = next(iter(train_loader))
            imgs, _ = test_batch
            imgs = imgs.to(device)
            recons, seeds = model(imgs[:4])
            
            # Test actual compression
            original_sizes = []
            compressed_sizes = []
            compression_ratios = []
            
            for i in range(4):
                # Original image size (theoretical)
                original_size_bytes = 3 * 128 * 128 * 4  # 32-bit floats
                
                # Compress to bytes
                compressed_bytes = compress_to_bytes(seeds[i], quantize_bits)
                compressed_size_bytes = len(compressed_bytes)
                
                original_sizes.append(original_size_bytes)
                compressed_sizes.append(compressed_size_bytes)
                compression_ratios.append(original_size_bytes / compressed_size_bytes)
            
            # Display results
            st.write("**Real Compression Results:**")
            cols = st.columns(4)
            
            for i in range(4):
                with cols[i]:
                    # Original
                    orig_img = imgs[i].cpu().permute(1, 2, 0).numpy()
                    st.image(orig_img, use_column_width=True, caption="Original")
                    
                    # Reconstructed
                    recon_img = recons[i].cpu().permute(1, 2, 0).numpy()
                    st.image(recon_img, use_column_width=True, caption="Reconstructed")
                    
                    # Metrics
                    loss = F.l1_loss(recons[i:i+1], imgs[i:i+1]).item()
                    st.caption(f"Error: {loss:.4f}")
                    st.caption(f"Compression: {compression_ratios[i]:.1f}:1")
                    st.caption(f"Size: {compressed_sizes[i]} bytes")
            
            # Statistics
            avg_ratio = np.mean(compression_ratios)
            avg_size = np.mean(compressed_sizes)
            st.success(f"ðŸ“Š Average: {avg_ratio:.1f}:1 compression, {avg_size:.1f} bytes per image")
            
            if avg_ratio > 100:
                st.balloons()
    
    # Next steps
    st.markdown("---")
    st.markdown('<div class="sub-header">ðŸš€ Ready for Real Datasets</div>', unsafe_allow_html=True)
    
    st.info("""
    **This model is now ready for:**
    - âœ… **CIFAR-10** (32x32 â†’ 128x128 upgrade path)
    - âœ… **CelebA Faces** (Human faces at 128x128)
    - âœ… **ImageNet Tiny** (200 categories, 64x64)
    - âœ… **Custom Datasets** (Your own 128x128 images)
    
    **Next Architecture Upgrades:**
    - VAE for better generalization
    - Attention mechanisms
    - Multi-scale processing
    - Perceptual loss functions
    """)

if __name__ == "__main__":
    main()